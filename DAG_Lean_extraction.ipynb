{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y curl build-essential\n",
        "\n",
        "# Install Lean 4 via elan\n",
        "!curl https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh -sSf | sh -s -- -y\n",
        "\n",
        "# Add Lean to PATH for the current session\n",
        "import os\n",
        "os.environ['PATH'] += \":/root/.elan/bin\"\n",
        "\n",
        "# Check Lean version\n",
        "!lean --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_lqE8a7LXsV",
        "outputId": "264f20c6-a475-45af-9447-98848fd657a9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://cli.github.com/packages stable/main amd64 Packages [357 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,776 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,910 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,107 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,613 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,861 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,302 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,626 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,767 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Fetched 37.7 MB in 8s (4,879 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "The following additional packages will be installed:\n",
            "  libcurl4 libcurl4-openssl-dev\n",
            "Suggested packages:\n",
            "  libcurl4-doc libidn11-dev libldap2-dev librtmp-dev\n",
            "The following packages will be upgraded:\n",
            "  curl libcurl4 libcurl4-openssl-dev\n",
            "3 upgraded, 0 newly installed, 0 to remove and 94 not upgraded.\n",
            "Need to get 870 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security/main amd64 libcurl4-openssl-dev amd64 7.81.0-1ubuntu1.22 [386 kB]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security/main amd64 curl amd64 7.81.0-1ubuntu1.22 [194 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security/main amd64 libcurl4 amd64 7.81.0-1ubuntu1.22 [290 kB]\n",
            "Fetched 870 kB in 1s (656 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 117540 files and directories currently installed.)\n",
            "Preparing to unpack .../libcurl4-openssl-dev_7.81.0-1ubuntu1.22_amd64.deb ...\n",
            "Unpacking libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.22) over (7.81.0-1ubuntu1.21) ...\n",
            "Preparing to unpack .../curl_7.81.0-1ubuntu1.22_amd64.deb ...\n",
            "Unpacking curl (7.81.0-1ubuntu1.22) over (7.81.0-1ubuntu1.21) ...\n",
            "Preparing to unpack .../libcurl4_7.81.0-1ubuntu1.22_amd64.deb ...\n",
            "Unpacking libcurl4:amd64 (7.81.0-1ubuntu1.22) over (7.81.0-1ubuntu1.21) ...\n",
            "Setting up libcurl4:amd64 (7.81.0-1ubuntu1.22) ...\n",
            "Setting up curl (7.81.0-1ubuntu1.22) ...\n",
            "Setting up libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.22) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[1minfo: \u001b[mdefault toolchain set to 'stable'\n",
            "\u001b[1minfo: \u001b[mdownloading https://releases.lean-lang.org/lean4/v4.28.0/lean-4.28.0-linux.tar.zst\n",
            "496.8 MiB / 496.8 MiB (100 %) 281.0 MiB/s ETA:   0 s\n",
            "\u001b[1minfo: \u001b[minstalling /root/.elan/toolchains/leanprover--lean4---v4.28.0\n",
            "Lean (version 4.28.0, x86_64-unknown-linux-gnu, commit 7e01a1bf5c70fc6167d49c345d3bf80596e9a79b, Release)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# 1. Create the Lake configuration file (lakefile.lean)\n",
        "# This tells Lean that \"Main\" and \"Solution\" are libraries in this project.\n",
        "with open(\"lakefile.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Lake\n",
        "open Lake DSL\n",
        "\n",
        "package my_proofs\n",
        "\n",
        "@[default_target]\n",
        "lean_lib Main\n",
        "\n",
        "@[default_target]\n",
        "lean_lib Solution\n",
        "\n",
        "-- Define the extractor executable\n",
        "lean_exe extract where\n",
        "  root := `Extract\n",
        "  supportInterpreter := true -- Required to access the Lean compiler internals\n",
        "\"\"\")\n",
        "\n",
        "# 2. Create Main.lean (Capitalized)\n",
        "with open(\"Main.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "def Goal_1 : Prop :=\n",
        "  ∀ n : Nat, 0 < n → 3 ∣ n ^ 2 → 3 ∣ n\n",
        "\n",
        "-- proof left as sorry\n",
        "theorem Goal_1_proof : Goal_1 := by\n",
        "  sorry\n",
        "\"\"\")\n",
        "\n",
        "# 3. Create Solution.lean (Capitalized, imports Main)\n",
        "with open(\"Solution.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Main\n",
        "\n",
        "theorem Solution : Goal_1 := by\n",
        "  intro n hpos hdiv\n",
        "  -- Since 3 is prime, if 3 divides n * n, it divides n\n",
        "  exact Nat.prime.dvd_of_dvd_mul (by norm_num) hdiv\n",
        "\"\"\")\n",
        "\n",
        "# 4. Build the project\n",
        "# This compiles Main.lean first, then checks Solution.lean\n",
        "print(\"Building project...\")\n",
        "result = subprocess.run([\"lake\", \"build\"], capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"✅ Build Successful! Main was imported into Solution correctly.\")\n",
        "else:\n",
        "    print(\"❌ Build Failed:\")\n",
        "    print(result.stderr)\n",
        "    print(result.stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN-V83gKrLif",
        "outputId": "ae4d1250-7a19-478c-cf0a-d1921d2c0e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building project...\n",
            "❌ Build Failed:\n",
            "error: build failed\n",
            "\n",
            "⚠ [2/5] Built Main (337ms)\n",
            "warning: Main.lean:6:8: declaration uses `sorry`\n",
            "✖ [4/5] Building Solution (310ms)\n",
            "trace: .> LEAN_PATH=/content/.lake/build/lib/lean /root/.elan/toolchains/leanprover--lean4---v4.28.0/bin/lean /content/Solution.lean -o /content/.lake/build/lib/lean/Solution.olean -i /content/.lake/build/lib/lean/Solution.ilean -c /content/.lake/build/ir/Solution.c --setup /content/.lake/build/ir/Solution.setup.json --json\n",
            "error: Solution.lean:7:38: unknown tactic\n",
            "error: Solution.lean:7:8: Unknown constant `Nat.prime.dvd_of_dvd_mul`\n",
            "error: Lean exited with code 1\n",
            "Some required targets logged failures:\n",
            "- Solution\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lake env lean Solution.lean"
      ],
      "metadata": {
        "id": "rjwLHmPbrmbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Function"
      ],
      "metadata": {
        "id": "Hga6y2_ysz2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"Extract.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Lean\n",
        "open Lean Elab Frontend Meta Json\n",
        "\n",
        "structure DeclInfo where\n",
        "  name : String\n",
        "  type : String\n",
        "  proofTerm : String\n",
        "  dependencies : List String\n",
        "  deriving ToJson\n",
        "\n",
        "unsafe def main (args : List String) : IO UInt32 := do\n",
        "  if args.length != 1 then\n",
        "    IO.println \"Usage: lake exe extract <filename.lean>\"\n",
        "    return 1\n",
        "\n",
        "  let fileName := args.head!\n",
        "  let input ← IO.FS.readFile fileName\n",
        "\n",
        "  let sysroot ← Lean.findSysroot\n",
        "  let leanPath ← IO.getEnv \"LEAN_PATH\"\n",
        "  let sp : System.SearchPath := match leanPath with\n",
        "    | some p => System.SearchPath.parse p\n",
        "    | none => []\n",
        "  Lean.initSearchPath sysroot sp\n",
        "\n",
        "  let inputCtx := Parser.mkInputContext input fileName\n",
        "  let (header, parserState, messages) ← Parser.parseHeader inputCtx\n",
        "  let (env, messages) ← processHeader header {} messages inputCtx\n",
        "\n",
        "  let cmdState := Command.mkState env messages {}\n",
        "  let frontendState ← IO.processCommands inputCtx parserState cmdState\n",
        "  let env := frontendState.commandState.env\n",
        "\n",
        "  let mut decls : List DeclInfo := []\n",
        "  let fileMap := FileMap.ofString input\n",
        "\n",
        "  -- FIX: Aggressive Pretty Printing Options to stop \"⋯\"\n",
        "  let opts : Options := Options.empty\n",
        "    |>.insert `pp.maxDepth (DataValue.ofNat 20000)\n",
        "    |>.insert `pp.width (DataValue.ofNat 180)\n",
        "    |>.insert `pp.deepTerms (DataValue.ofBool false)  -- Disable deep term truncation\n",
        "    |>.insert `pp.proofs (DataValue.ofBool true)      -- Print full proofs\n",
        "    |>.insert `pp.minSteps (DataValue.ofNat 10000)    -- Increase stepping limits\n",
        "\n",
        "  let coreCtx : Core.Context := { fileName := fileName, fileMap := fileMap, options := opts }\n",
        "  let coreSt : Core.State := { env := env }\n",
        "\n",
        "  let constants := env.constants.map₂\n",
        "\n",
        "  for (name, cinfo) in constants do\n",
        "    let nameStr := name.toString\n",
        "\n",
        "    -- STRICT FILTER: Ignore internal match helpers completely.\n",
        "    -- If 'lemma2' prints as 'match ...', we don't need 'lemma2.match_1'.\n",
        "    let isInternal := nameStr.contains \"match_\" || nameStr.contains \"proof_\" || name.isInternal\n",
        "\n",
        "    if (env.getModuleIdxFor? name).isNone && !isInternal then\n",
        "      if cinfo.hasValue then\n",
        "        try\n",
        "          let (typeStr, valStr, deps) ← (MetaM.run' <| do\n",
        "              let type ← ppExpr cinfo.type\n",
        "              let valExpr := cinfo.value?.getD (Expr.sort Level.zero)\n",
        "              let valPp ← ppExpr valExpr\n",
        "\n",
        "              -- Clean Dependencies\n",
        "              let used := valExpr.getUsedConstants\n",
        "              let validDeps := used.toList.filterMap fun n =>\n",
        "                let nStr := n.toString\n",
        "                -- Don't list matchers as dependencies either\n",
        "                if !nStr.contains \"match_\" && !nStr.contains \"proof_\" then\n",
        "                  some nStr\n",
        "                else\n",
        "                  none\n",
        "\n",
        "              return (type.pretty, valPp.pretty, validDeps)\n",
        "          ).run' coreCtx coreSt |>.toIO (fun _ => IO.userError \"Meta error\")\n",
        "\n",
        "          decls := decls.concat {\n",
        "            name := nameStr,\n",
        "            type := typeStr,\n",
        "            proofTerm := valStr,\n",
        "            dependencies := deps\n",
        "          }\n",
        "        catch _ => pure ()\n",
        "\n",
        "  let json := Json.mkObj [\n",
        "    (\"success\", true),\n",
        "    (\"module\", env.mainModule.toString),\n",
        "    (\"declarations\", toJson decls)\n",
        "  ]\n",
        "\n",
        "  IO.println json.pretty\n",
        "  return 0\n",
        "\"\"\")\n",
        "\n",
        "# Rebuild and Extract\n",
        "import subprocess\n",
        "print(\"Building Extractor...\")\n",
        "subprocess.run([\"lake\", \"build\", \"extract\"], check=True)\n",
        "\n",
        "print(\"Running Extraction...\")\n",
        "res = subprocess.run([\"lake\", \"exe\", \"extract\", \"Solution.lean\"], capture_output=True, text=True)\n",
        "\n",
        "if res.returncode == 0:\n",
        "    print(\"✅ JSON Generated.\")\n",
        "    with open(\"extracted.json\", \"w\") as f:\n",
        "        f.write(res.stdout)\n",
        "else:\n",
        "    print(\"❌ Extraction Failed:\")\n",
        "    print(res.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfxgzuyar4LZ",
        "outputId": "ecea9ccd-4c82-4ebc-f66a-393b39d8ccbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Extractor...\n",
            "Running Extraction...\n",
            "✅ JSON Generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lake build extract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euc3qS26tM0f",
        "outputId": "d27cfc9e-2f28-438b-f3dc-47726b8e8bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31merror:\u001b[m unknown target `extract`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rebuild proof"
      ],
      "metadata": {
        "id": "iOS-oPenoBVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import networkx as nx\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "m9C3DNGLJD5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rebuild_proof(original_main, json_path, target_goal, output_file):\n",
        "    print(f\"--- Processing {json_path} ---\")\n",
        "\n",
        "    # 1. Parse Original Main.lean to get the 'Preamble'\n",
        "    # We keep everything up to the definition of the target theorem\n",
        "    preamble_lines = []\n",
        "    found_target = False\n",
        "\n",
        "    if not os.path.exists(original_main):\n",
        "        print(f\"Error: {original_main} not found.\")\n",
        "        return\n",
        "\n",
        "    with open(original_main, 'r') as f:\n",
        "        for line in f:\n",
        "            # Stop when we hit the theorem we are replacing\n",
        "            if line.strip().startswith(f\"theorem {target_goal}\"):\n",
        "                found_target = True\n",
        "                break\n",
        "            preamble_lines.append(line)\n",
        "\n",
        "    if not found_target:\n",
        "        print(f\"Warning: Could not find 'theorem {target_goal}' in {original_main}.\")\n",
        "        # We proceed anyway, assuming the user might want to append to the end\n",
        "\n",
        "    preamble = \"\".join(preamble_lines)\n",
        "\n",
        "    # 2. Load JSON\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 3. Build Graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Helper to map temporary solution name to final target name\n",
        "    def get_final_name(n):\n",
        "        return target_goal if n == f\"{target_goal}_solution\" else n\n",
        "\n",
        "    # Store code snippets in the graph nodes\n",
        "    defined_nodes = set()\n",
        "\n",
        "    for decl in data['declarations']:\n",
        "        raw_name = decl['name']\n",
        "        name = get_final_name(raw_name)\n",
        "\n",
        "        # Determine if theorem or def\n",
        "        # (Simple heuristic: proofs of Props are theorems)\n",
        "        kind = \"theorem\" if \"Prop\" in decl['type'] or \"Even\" in decl['type'] else \"def\"\n",
        "\n",
        "        # Generate the Lean code block\n",
        "        code = f\"{kind} {name} : {decl['type']} := {decl['proofTerm']}\"\n",
        "\n",
        "        G.add_node(name, code=code)\n",
        "        defined_nodes.add(name)\n",
        "\n",
        "        # Add edges for dependencies\n",
        "        for dep in decl['dependencies']:\n",
        "            dep_name = get_final_name(dep)\n",
        "            # Edge direction: Dependency -> Node\n",
        "            # (Because Node depends on Dependency, so Dependency must come first)\n",
        "            G.add_edge(dep_name, name)\n",
        "\n",
        "    # 4. Prune the Graph\n",
        "    # We only want nodes that 'target_goal' relies on.\n",
        "    if target_goal in G:\n",
        "        # Get all ancestors (dependencies) of the target\n",
        "        relevant_nodes = nx.ancestors(G, target_goal)\n",
        "        relevant_nodes.add(target_goal) # Include the target itself\n",
        "\n",
        "        # Create subgraph\n",
        "        G_pruned = G.subgraph(relevant_nodes).copy()\n",
        "        print(f\"Graph Pruned: {len(G)} nodes -> {len(G_pruned)} nodes.\")\n",
        "\n",
        "        # Check what was dropped\n",
        "        dropped = set(defined_nodes) - set(G_pruned.nodes)\n",
        "        if dropped:\n",
        "            print(f\"Dropped unused lemmas: {dropped}\")\n",
        "    else:\n",
        "        print(f\"Error: Target {target_goal} not found in generated proofs.\")\n",
        "        return\n",
        "\n",
        "    # 5. Visualization\n",
        "    print(\"\\n--- Dependency Tree ---\")\n",
        "    plot_knowledge_graph(G_pruned, target_goal)\n",
        "    print(\"-----------------------\\n\")\n",
        "\n",
        "    # 6. Topological Sort for compilation order\n",
        "    try:\n",
        "        sorted_nodes = list(nx.topological_sort(G_pruned))\n",
        "    except nx.NetworkXUnfeasible:\n",
        "        print(\"Error: Cycle detected in dependencies.\")\n",
        "        return\n",
        "\n",
        "    # 7. Write Result\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(\"-- GENERATED PROOF FILE\\n\")\n",
        "        # Ensure we have standard imports\n",
        "        if \"import Lean\" not in preamble:\n",
        "            f.write(\"import Lean\\n\")\n",
        "\n",
        "        f.write(preamble.strip() + \"\\n\\n\")\n",
        "        f.write(\"/-- Reconstructed Proofs (Topologically Sorted) --/\\n\\n\")\n",
        "\n",
        "        for node in sorted_nodes:\n",
        "            # Only write code for nodes we actually defined in the JSON\n",
        "            # (Ignore external nodes like 'Nat', 'Even' that are in the graph as deps)\n",
        "            if node in G_pruned.nodes and \"code\" in G_pruned.nodes[node]:\n",
        "                f.write(G_pruned.nodes[node][\"code\"] + \"\\n\\n\")\n",
        "\n",
        "    print(f\"✅ Successfully generated {output_file}\")"
      ],
      "metadata": {
        "id": "CNRYpHeWfUzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_knowledge_graph(G, target_goal):\n",
        "    \"\"\"\n",
        "    Visualizes the dependency graph using Matplotlib.\n",
        "    Only plots the nodes defined in the new proof (the \"knowledge graph\").\n",
        "    \"\"\"\n",
        "    # 1. Filter the graph to only include nodes we defined (those with 'code')\n",
        "    # This removes external dependencies like 'Nat', 'Even', etc. from the plot.\n",
        "    plot_nodes = [n for n, data in G.nodes(data=True) if 'code' in data]\n",
        "    plot_G = G.subgraph(plot_nodes)\n",
        "\n",
        "    if plot_G.number_of_nodes() == 0:\n",
        "        print(\"Plotting skipped: No new internal dependencies to visualize.\")\n",
        "        return\n",
        "\n",
        "    # 2. Set up colors and layout\n",
        "    node_colors = []\n",
        "    for node in plot_G.nodes():\n",
        "        if node == target_goal:\n",
        "            node_colors.append('skyblue')  # The main goal\n",
        "        else:\n",
        "            node_colors.append('lightgreen')  # Supporting lemmas\n",
        "\n",
        "    # Use a layout that spreads nodes out\n",
        "    pos = nx.spring_layout(plot_G, k=1.5, iterations=50, seed=42)\n",
        "\n",
        "    # 3. Draw the graph\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    nx.draw_networkx_nodes(plot_G, pos, node_size=3000, node_color=node_colors, edgecolors='black')\n",
        "    nx.draw_networkx_edges(plot_G, pos,\n",
        "                           arrowstyle='->',\n",
        "                           arrowsize=20,\n",
        "                           node_size=3000,\n",
        "                           width=1.5,\n",
        "                           connectionstyle='arc3,rad=0.1')\n",
        "    nx.draw_networkx_labels(plot_G, pos, font_size=10, font_weight='bold')\n",
        "\n",
        "    # 4. Display the plot\n",
        "    plt.title(\"Proof Dependency Graph\", size=15)\n",
        "    plt.axis('off')  # Hide the axes\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gvQU4zzVfYcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Function"
      ],
      "metadata": {
        "id": "LQVjWfeeoIo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rebuild_proof(\n",
        "        original_main=\"Main.lean\",\n",
        "        json_path=\"extracted.json\",\n",
        "        target_goal=\"Goal_1_proof\",\n",
        "        output_file=\"generatedMain.lean\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdIiBqdBfhkA",
        "outputId": "7c440641-f31a-4fe0-dc55-47d1796677ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing extracted.json ---\n",
            "Error: Target Goal_1_proof not found in generated proofs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lake env lean generatedMain.lean"
      ],
      "metadata": {
        "id": "QPe_pMseyBb1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}