{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFhoZ7U3cAM6"
      },
      "source": [
        "# Lean splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OybECbEhfP6-",
        "outputId": "77c06878-cc68-46ab-9014-187213655ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building updated extractor executable...\n",
            "‚úÖ Extractor tool is updated and ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Create and Build the Extractor\n",
        "import os\n",
        "\n",
        "with open(\"Extract.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Lean\n",
        "open Lean Elab Frontend Meta Json\n",
        "\n",
        "structure DeclInfo where\n",
        "  name : String\n",
        "  type : String\n",
        "  proofTerm : String\n",
        "  dependencies : List String\n",
        "  isProp : Bool\n",
        "  isAbbrev : Bool  -- NEW: Tracks if this is an 'abbrev'\n",
        "  deriving ToJson\n",
        "\n",
        "-- HELPER: Properly check for substrings\n",
        "def containsSubstr (s substr : String) : Bool :=\n",
        "  (s.splitOn substr).length > 1\n",
        "\n",
        "unsafe def main (args : List String) : IO UInt32 := do\n",
        "  if args.length != 1 then\n",
        "    IO.println \"Usage: lake exe extract <filename.lean>\"\n",
        "    return 1\n",
        "\n",
        "  let fileName := args.head!\n",
        "  let input ‚Üê IO.FS.readFile fileName\n",
        "\n",
        "  let sysroot ‚Üê Lean.findSysroot\n",
        "  let leanPath ‚Üê IO.getEnv \"LEAN_PATH\"\n",
        "  let sp : System.SearchPath := match leanPath with\n",
        "    | some p => System.SearchPath.parse p\n",
        "    | none => []\n",
        "  Lean.initSearchPath sysroot sp\n",
        "\n",
        "  let inputCtx := Parser.mkInputContext input fileName\n",
        "  let (header, parserState, messages) ‚Üê Parser.parseHeader inputCtx\n",
        "  let (env, messages) ‚Üê processHeader header {} messages inputCtx\n",
        "\n",
        "  let cmdState := Command.mkState env messages {}\n",
        "  let frontendState ‚Üê IO.processCommands inputCtx parserState cmdState\n",
        "  let env := frontendState.commandState.env\n",
        "\n",
        "  let mut decls : List DeclInfo := []\n",
        "  let fileMap := FileMap.ofString input\n",
        "\n",
        "  let opts : Options := Options.empty\n",
        "    |>.insert `pp.maxDepth (DataValue.ofNat 20000)\n",
        "    |>.insert `pp.width (DataValue.ofNat 1000)\n",
        "    |>.insert `pp.deepTerms (DataValue.ofBool false)\n",
        "    |>.insert `pp.explicit (DataValue.ofBool true)   -- CRITICAL: Prints @Nat.le_trans 11 21 ...\n",
        "    |>.insert `pp.fullNames (DataValue.ofBool true)  -- CRITICAL: Avoids naming ambiguity\n",
        "    |>.insert `pp.analyze (DataValue.ofBool true)    -- CRITICAL: Adds type annotations\n",
        "\n",
        "  let coreCtx : Core.Context := { fileName := fileName, fileMap := fileMap, options := opts }\n",
        "  let coreSt : Core.State := { env := env }\n",
        "\n",
        "  let constants := env.constants.map‚ÇÇ\n",
        "\n",
        "  for (name, cinfo) in constants do\n",
        "    let nameStr := name.toString\n",
        "\n",
        "    let isInternal := containsSubstr nameStr \"match_\" ||\n",
        "                      containsSubstr nameStr \"proof_\" ||\n",
        "                      containsSubstr nameStr \".eq_\" ||\n",
        "                      containsSubstr nameStr \".injEq\" ||\n",
        "                      name.isInternal\n",
        "\n",
        "    if (env.getModuleIdxFor? name).isNone && !isInternal then\n",
        "      if cinfo.hasValue then\n",
        "        try\n",
        "          let (typeStr, valStr, deps, isP) ‚Üê (MetaM.run' <| do\n",
        "              let type ‚Üê ppExpr cinfo.type\n",
        "              let valExpr := cinfo.value?.getD (Expr.sort Level.zero)\n",
        "              let valPp ‚Üê ppExpr valExpr\n",
        "              let used := valExpr.getUsedConstants\n",
        "\n",
        "              let validDeps := used.toList.filterMap fun n =>\n",
        "                let nStr := n.toString\n",
        "                if !(containsSubstr nStr \"match_\") && !(containsSubstr nStr \"proof_\") && !(containsSubstr nStr \".eq_\") then\n",
        "                  some nStr\n",
        "                else\n",
        "                  none\n",
        "\n",
        "              let isP ‚Üê Meta.isProp cinfo.type\n",
        "              return (type.pretty, valPp.pretty, validDeps, isP)\n",
        "          ).run' coreCtx coreSt |>.toIO (fun _ => IO.userError \"Meta error\")\n",
        "\n",
        "          -- CHECK IF ABBREV: Look at the ReducibilityHints\n",
        "          let isAbbrev := match cinfo.hints with\n",
        "            | ReducibilityHints.abbrev => true\n",
        "            | _ => false\n",
        "\n",
        "          decls := decls.concat {\n",
        "            name := nameStr,\n",
        "            type := typeStr,\n",
        "            proofTerm := valStr,\n",
        "            dependencies := deps,\n",
        "            isProp := isP,\n",
        "            isAbbrev := isAbbrev\n",
        "          }\n",
        "        catch _ => pure ()\n",
        "\n",
        "  let json := Json.mkObj [\n",
        "    (\"success\", Json.bool true),\n",
        "    (\"module\", Json.str env.mainModule.toString),\n",
        "    (\"declarations\", toJson decls)\n",
        "  ]\n",
        "\n",
        "  IO.println json.pretty\n",
        "  return 0\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a minimal lakefile just to build the extractor tool\n",
        "with open(\"lakefile.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Lake\n",
        "open Lake DSL\n",
        "\n",
        "package extractor_tool\n",
        "\n",
        "lean_exe extract where\n",
        "  root := `Extract\n",
        "  supportInterpreter := true\n",
        "\"\"\")\n",
        "\n",
        "print(\"Building updated extractor executable...\")\n",
        "res = subprocess.run([\"lake\", \"build\", \"extract\"], capture_output=True, text=True)\n",
        "if res.returncode == 0:\n",
        "    print(\"‚úÖ Extractor tool is updated and ready.\")\n",
        "else:\n",
        "    print(\"‚ùå Build failed:\")\n",
        "    print(res.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEDNeoIafXQ-"
      },
      "source": [
        "## 2.2 Splitter  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xepCsQqSfea1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import tempfile\n",
        "import networkx as nx\n",
        "from pathlib import Path\n",
        "\n",
        "def run_extractor_on_file(input_path):\n",
        "    # remove content if not in collab\n",
        "    exe_paths =[\"/content/.lake/build/bin/extract\", \"/content/build/bin/extract\"]\n",
        "    extract_exe = next((p for p in exe_paths if os.path.exists(p)), None)\n",
        "\n",
        "    if not extract_exe:\n",
        "        raise FileNotFoundError(\"Could not find 'extract' executable.\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        temp_path = Path(temp_dir)\n",
        "        input_filename = Path(input_path).name\n",
        "        module_name = Path(input_filename).stem.capitalize()\n",
        "\n",
        "        shutil.copy(input_path, temp_path / f\"{module_name}.lean\")\n",
        "\n",
        "        lakefile_content = f\"import Lake\\nopen Lake DSL\\npackage temp_project\\n@[default_target]\\nlean_lib {module_name}\\n\"\n",
        "        (temp_path / \"lakefile.lean\").write_text(lakefile_content)\n",
        "\n",
        "        result = subprocess.run([\"lake\", \"env\", extract_exe, f\"{module_name}.lean\"],\n",
        "            cwd=temp_dir, capture_output=True, text=True, check=True\n",
        "        )\n",
        "        return json.loads(result.stdout)\n",
        "\n",
        "\n",
        "def split_lean_project(input_file, main_goal_name, output_dir):\n",
        "    print(f\"--- Starting Lean Project Split ---\")\n",
        "    data = run_extractor_on_file(input_file)\n",
        "\n",
        "    # ==========================================\n",
        "    # GRAPH WITH PRUNING PHASE\n",
        "    # ==========================================\n",
        "    G = nx.DiGraph()\n",
        "    extracted_names = {decl['name'] for decl in data['declarations']}\n",
        "\n",
        "    # 1. Build the graph\n",
        "    for decl in data['declarations']:\n",
        "        name = decl['name']\n",
        "        G.add_node(name)\n",
        "        for dep in decl['dependencies']:\n",
        "            # Direction: Dependency -> Node that uses it\n",
        "            G.add_edge(dep, name)\n",
        "\n",
        "    if main_goal_name not in extracted_names:\n",
        "        print(f\"‚ùå Error: Main goal '{main_goal_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    # 2. Find all ancestors of the main goal (everything it depends on, directly or indirectly)\n",
        "    required_nodes = nx.ancestors(G, main_goal_name)\n",
        "    required_nodes.add(main_goal_name) # Keep the goal itself!\n",
        "\n",
        "    # 3. Filter the declarations\n",
        "    pruned_declarations =[]\n",
        "    dropped_nodes = []\n",
        "\n",
        "    for decl in data['declarations']:\n",
        "        if decl['name'] in required_nodes:\n",
        "            pruned_declarations.append(decl)\n",
        "        else:\n",
        "            dropped_nodes.append(decl['name'])\n",
        "\n",
        "    if dropped_nodes:\n",
        "        print(f\"‚úÇÔ∏è  Pruned unused declarations (dead code): {dropped_nodes}\")\n",
        "    # ==========================================\n",
        "\n",
        "    definitions = {}\n",
        "    lemmas = {}\n",
        "    main_goal = None\n",
        "\n",
        "    # Use the PRUNED list instead of the full data\n",
        "    for decl in pruned_declarations:\n",
        "        name = decl['name']\n",
        "        if name == main_goal_name:\n",
        "            main_goal = decl\n",
        "        elif decl.get('isProp', False):\n",
        "            lemmas[name] = decl\n",
        "        else:\n",
        "            definitions[name] = decl\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Write Defs.lean ---\n",
        "    if definitions:\n",
        "        G_defs = nx.DiGraph()\n",
        "        for name, decl in definitions.items():\n",
        "            G_defs.add_node(name)\n",
        "            for dep in decl['dependencies']:\n",
        "                if dep in definitions:\n",
        "                    G_defs.add_edge(dep, name)\n",
        "\n",
        "        sorted_defs = list(nx.topological_sort(G_defs))\n",
        "\n",
        "        with open(os.path.join(output_dir, 'Defs.lean'), 'w') as f:\n",
        "            f.write(\"import Lean\\n\\n\")\n",
        "            for name in sorted_defs:\n",
        "                decl = definitions[name]\n",
        "                kind = \"abbrev\" if decl.get('isAbbrev', False) else \"def\"\n",
        "                f.write(f\"{kind} {name} : {decl['type']} := {decl['proofTerm']}\\n\\n\")\n",
        "\n",
        "    # --- Write individual lemma files ---\n",
        "    for name, decl in lemmas.items():\n",
        "        lemma_filename = name.capitalize() + \".lean\"\n",
        "        with open(os.path.join(output_dir, lemma_filename), 'w') as f:\n",
        "            imports = {\"import Lean\"}\n",
        "            for dep in decl['dependencies']:\n",
        "                if dep in definitions: imports.add(\"import Defs\")\n",
        "                elif dep in lemmas: imports.add(f\"import {dep.capitalize()}\")\n",
        "            f.write(\"\\n\".join(sorted(list(imports))) + \"\\n\\n\")\n",
        "            f.write(f\"theorem {name} : {decl['type']} := {decl['proofTerm']}\\n\")\n",
        "\n",
        "    # --- Write Main.lean ---\n",
        "    with open(os.path.join(output_dir, 'Main.lean'), 'w') as f:\n",
        "        imports = {\"import Lean\"}\n",
        "        for dep in main_goal['dependencies']:\n",
        "            if dep in definitions: imports.add(\"import Defs\")\n",
        "            elif dep in lemmas: imports.add(f\"import {dep.capitalize()}\")\n",
        "        f.write(\"\\n\".join(sorted(list(imports))) + \"\\n\\n\")\n",
        "        f.write(f\"theorem {main_goal['name']} : {main_goal['type']} := {main_goal['proofTerm']}\\n\")\n",
        "\n",
        "    # --- Generate lakefile.lean ---\n",
        "    with open(os.path.join(output_dir, 'lakefile.lean'), 'w') as f:\n",
        "        f.write(\"import Lake\\nopen Lake DSL\\n\\npackage split_project\\n\\n\")\n",
        "        f.write(\"@[default_target]\\nlean_lib Main\\n\")\n",
        "        if definitions: f.write(\"lean_lib Defs\\n\")\n",
        "        for name in lemmas: f.write(f\"lean_lib {name.capitalize()}\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Split complete! Files created in: /content/{output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88t6ouvxf28t"
      },
      "source": [
        "# 2.3 Define Monolitic proof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNrrFQHEfzKZ"
      },
      "outputs": [],
      "source": [
        "# 1. Create the monolithic input file\n",
        "with open(\"monolith.lean\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import Lean\n",
        "\n",
        "abbrev MyNumberType := Nat\n",
        "\n",
        "def is_special (n : MyNumberType) : Prop := n > 10\n",
        "\n",
        "-- This one has a real proof\n",
        "theorem lemma1 (n : MyNumberType) (h : n > 20) : is_special n :=\n",
        "  by simp [is_special]; exact Nat.le_trans (by decide) h\n",
        "\n",
        "-- This one is left not as sorry\n",
        "theorem lemma2 (n : MyNumberType) : n = n :=\n",
        "  rfl\n",
        "\n",
        "-- The main goal has a real proof\n",
        "theorem main_goal (k : MyNumberType) (h : k > 20) : is_special k :=\n",
        "  lemma1 k h\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrxll5dyfiIA",
        "outputId": "011aecf9-5fc4-455c-ad75-afdc3aef4d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Lean Project Split ---\n",
            "‚úÇÔ∏è  Pruned unused declarations (dead code): ['lemma2']\n",
            "‚úÖ Split complete! Files created in: /content/myproj8\n"
          ]
        }
      ],
      "source": [
        "# 2. Run the splitter\n",
        "split_lean_project(\n",
        "    input_file=\"monolith.lean\",\n",
        "    main_goal_name=\"main_goal\",\n",
        "    output_dir=\"myproj8\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRbJmKnJ0g2N",
        "outputId": "47224c9b-a803-432b-f521-5846e8e07233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- üìÑ Lemma1.lean (Should have a real proof) ---\n",
            "import Defs\n",
            "import Lean\n",
            "\n",
            "theorem lemma1 : ‚àÄ (n : MyNumberType), n > 20 ‚Üí is_special n := fun n h => Eq.mpr (id gt_iff_lt._simp_1) (Nat.le_trans (of_decide_eq_true (id (Eq.refl true))) h)\n",
            "\n",
            "--- üìÑ Main.lean (Should have a real proof) ---\n",
            "import Defs\n",
            "import Lean\n",
            "import Lemma1\n",
            "\n",
            "theorem main_goal : ‚àÄ (k : MyNumberType), k > 20 ‚Üí is_special k := fun k h => lemma1 k h\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Print the contents of the generated files to verify\n",
        "print(\"\\n--- üìÑ Lemma1.lean (Should have a real proof) ---\")\n",
        "with open(\"myproj7/Lemma1.lean\", \"r\") as f: print(f.read())\n",
        "\n",
        "print(\"--- üìÑ Main.lean (Should have a real proof) ---\")\n",
        "with open(\"myproj7/Main.lean\", \"r\") as f: print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qicZkx_hitS2"
      },
      "source": [
        "# Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_-7WHTLz7hb",
        "outputId": "129287a7-3049-4203-ac1d-ff74b0377e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Build Output ---\n",
            "\u001b[1;34minfo:\u001b[m split_project: no previous manifest, creating one from scratch\n",
            "\u001b[1;34minfo:\u001b[m toolchain not updated; no toolchain information found\n",
            "‚£æ [1/1] Running job computation (+ 0 more)\u001b[2K\n",
            "‚£∑ [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£Ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£ü [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚°ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚¢ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£ª [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£Ω [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£æ [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£∑ [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£Ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£ü [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚°ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚¢ø [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£ª [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£Ω [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£æ [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£∑ [2/5] Running Defs (+ 0 more)\u001b[2K\n",
            "‚£Ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£ü [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚°ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚¢ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£ª [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£Ω [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£æ [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£∑ [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£Ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£ü [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚°ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚¢ø [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£ª [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£Ω [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£æ [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£∑ [3/5] Running Lemma1 (+ 0 more)\u001b[2K\n",
            "‚£Ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£ü [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚°ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚¢ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£ª [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£Ω [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£æ [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£∑ [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£Ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£ü [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚°ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚¢ø [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "‚£ª [4/5] Running Main (+ 0 more)\u001b[2K\n",
            "Build completed successfully (5 jobs).\n",
            "\n",
            "‚úÖ Verification Successful!\n"
          ]
        }
      ],
      "source": [
        "# 1. Run the command and capture output into a variable\n",
        "output_lines = !cd myproj8 && lake build\n",
        "# 2. Convert the list of lines into a single string (like .stdout)\n",
        "stdout_string = \"\\n\".join(output_lines)\n",
        "# 3. Print or check the output\n",
        "print(\"--- Build Output ---\")\n",
        "print(stdout_string)\n",
        "#\n",
        "# 4. Check if it succeeded programmatically\n",
        "if \"Build completed successfully\" in stdout_string:\n",
        "    print(\"\\n‚úÖ Verification Successful!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Verification Failed or Warnings found.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
